问题一
mysql> create table t1(id int(1)primary key auto_increment,name char(10));
ERROR 1813 (HY000): Tablespace '`db3`.`t1`' exists.

[root@host50 db3]# ls
db.opt  t1.ibd

mysql> drop database db3;
ERROR 1010 (HY000): Error dropping database (can't rmdir './db3', errno: 39)
后手动删除再创db3库(在mysql里查看db3库是没内容，到/var/lib/mysql/db3下有个
t1.ibd）

mysql> create table db3.t1(id int(1) primary key auto_increment, name char(10));
ERROR 1030 (HY000): Got error 168 from storage engine
（没有t1表的存在创不了，创其除了“t1”外其他表名就可以）

mysql> use db1;
Database changed
mysql> show tables;
Empty set (0.00 sec)

mysql> create table t1(id int);
ERROR 1050 (42S01): Table '`db1`.`t1`' already exists

[root@host50 db1]# ls
db.opt  t1.ibd
(再重新建一个库再建t1表，建t1表不成功，去看db1库下自动有t1.ibd文件了)

!!!!!!!!!!!解决方法
     先在/var/lib/mysql/db3下把t1.ibd手动删除，再在mysql里drop掉db3库就再创建没问题了

问题二、
mysql> grant replication slave on *.* to repluser@'192.168.4.52' identified by '123456';这里面的repluser用户的作用是什么？除了用它配置从库内容后，用他查看主库的东西是查看不了。

问题三、（解决） 登录时加－A
Reading table information for completion of table and column names
You can turn off this feature to get a quicker startup with -A



问题四、（重新删掉后再创也一样，但在其他从库可以desc表？）
主库卡很久，且出现问题
mysql> desc bbsdb.a;
ERROR 2013 (HY000): Lost connection to MySQL server during query
但从库打这命令很快看到不卡

invalid    n.病人a无效的
*[in'vælid]

select name from user where sex="gril" and uid=(select max(uid) from user);
select name from user where sex="gril" order by uid desc limit1;

mysqldump 完全备份 锁表（写） 
binlog日志  增量备份
（重要）percona软件 提供 innobackupex命令 能完全备份或增量备份

show variables like "%password%";
 validate_password_policy=0
 validate_password_length=6 


incorrect
declare

重装mysql
ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/tmp/mysql.sock' (2)
解决mysql -uroot -p'R%N9xMLn!6Yg' -S /var/lib/mysql/mysql.sock
https://blog.csdn.net/hjf161105/article/details/78850658

问题rm -rf /etc/my.cnf /var/lib/mysql再装有冲突，就要
yum -y remove mysql

++++++++++++++++++++++++++++++++++++++++++++++++++++++
nosql
mongodb问题
# mongod -f /usr/local/mongodb/etc/mongodb.conf
about to fork child process, waiting until server is ready for connections.
forked process: 1907
ERROR: child process failed, exited with error number 1
解决方法:
删除mongodb 的db目录下的mongod.lock文件，重启mongodb即可



+++++++++++++++++++
mysql集群

问题一:(21主;22,23主从;24,25从库)
]# masterha_check_repl --conf=/etc/mha_manager/app1.cnf
Sat Aug  4 09:32:40 2018 - [error][/usr/local/share/perl5/MHA/ManagerUtil.pm, ln122] Got error when getting node version. Error:
Sat Aug  4 09:32:40 2018 - [error][/usr/local/share/perl5/MHA/ManagerUtil.pm, ln123] 
bash: apply_diff_relay_logs: command not found
Sat Aug  4 09:32:40 2018 - [error][/usr/local/share/perl5/MHA/ManagerUtil.pm, ln150] node version on 192.168.4.24 not found! Is MHA Node package installed ?
原因:查看集群中有某台主机未安装mha_node软件 

问题二
Last_IO_Error: Fatal error: The slave I/O thread stops because master and slave have equal MySQL server ids; these ids must be different for replication to work (or the --replicate-same-server-id option must be used on slave but this does not always make sense; please check the manual before using it).
原因:
1.先查主从机子的server_id和uuid是否相同
查看uuid方法 show variables like "%uuid%";或cat /var/lib/mysql/auto.cnf
查看server_id方法  select @@server_id;或cat /etc/my.cnf/里的server_id=*哪项
2.(谨记)以上不存在问题的话,就存在当主库down后,恢复添加主机后再change master to master_host=主库ip时,是把主库ip写错也会出现这情况


++++++++++++++++++++++++++++++++++++++++++
企业里的通线到少有2个运营商,电信联通居多,移动一般不选择
但原不同运营商服务不能互通信,但通过BGP持术可以实现

change master to master_host="192.168.4.21",master_user="a",aster_password="123qqq...A",master_log_file="db21.000004",master_log_pos=474;

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
nfs问题
问题一:rpcbind启动服务报错
]# systemctl start rpcbind
Job for rpcbind.service failed because the control process exited with error code. See "systemctl status rpcbind.service" and "journalctl -xe" for details.
]# journalctl -xe
8月 06 11:04:26 san71 setroubleshoot[3151]: failed to retrieve rpm info for /run/rpcbind.lock
8月 06 11:04:26 san71 setroubleshoot[3151]: SELinux is preventing /usr/sbin/rpcbind from lock access on the file
查看getenforce --->Permissive ,后rm -rf /run/rpcbind.lock 再启动rpcbind服务就可


++++++++++++++++++++++++++++++++++++++
IP问题
作LVS-DR调度时,在real server主机的lo本地网卡中,为什么201.1.1.101/32 和127.0.0.1/8的子网掩码设置为32和8的作用?

问题二
配DR集群时,如果访问vip地址卡时,看real server主机lo网卡有没有配本地vip,还有real server主机的/proc/sys/net/ipv4/conf/all/下的arp_announce和arp_ignore的值有没有分别设置为2和1


++++++++++++++++++++++++++++++++++++++++
ceph集群(主机名和时间同步要求到)
问题一
node1 ceph-cluster]# ceph-deploy mon create-initial
提示 write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.mon][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[ceph_deploy][ERROR ] GenericError: Failed to create 3 monitors
解决:把node1,node2,node3的/etc/ceph/ceph.conf,再执行ceph-deploy mon create-initial

问题二
[ceph_deploy.mon][INFO  ] mon.node3 monitor has reached quorum!
[ceph_deploy.mon][ERROR ] Some monitors have still not reached quorum:
[ceph_deploy.mon][ERROR ] node1
解决:检查防火墙,能否无密码shh远程,nodo1主机也要发密钥给自已
本人:把node1的/etc/hosts都同步到node2和node3

问题三
]# ceph-deploy osd create node1:vdc:/dev/vdb1 node1:vdd:/dev/vdb2
[ceph_deploy][ERROR ] RuntimeError: bootstrap-osd keyring not found; run 'gatherkeys'
解决:

问题四:[node1][ERROR ] File "/usr/lib64/python2.7/subprocess.py", line 1327, in _ex
解决:是ceph的yum源没做好
[MON  ]
name=MON  的格式[ MOD  ]这样yum是识别不了的,要[MON]才行,注意了



[ceph_deploy][ERROR ] RuntimeError: NoSectionError: No section: 'ceph'
方法: yum remove ceph-release





++++++++++++++++++++++++++++++++++++++++++++++++
iscsi问题
如果iscsi如果discover发现不了,可以采用lonin命令

+++++++++++++++++++++++++++++++++++++++++
yum问题
yum update
yum makecache 缓存yum源

问题一
/var/run/yum.pid已被锁定，PID为1610的另一个程序正在运行。
解决方法:rm -f /var/run/yum.pid删除这个文件再去yum即可

问题二  格式[MON  ]
[MON  ]
name=MON
baseurl=ftp://192.168.1.254/ceph/rhceph-2.0-rhel-7-x86_64/MON
gpgcheck=0
enalbed=1

++++++++++++++++++++++
apache   httpd

问题一
AH00558: httpd: Could not reliably determine the server's fully qualified domain name
解决:vim /etc/httpd/conf/httpd.conf
将里面的 #ServerName localhost:80 注释去掉即


+++++++++++++++++++++++++++++++++++++
nfs

问题一:docker02 ~]# showmount -e 192.168.1.13clnt_create: RPC: Port mapper failure - Unable to receive: errno 113 (No route to host)
解决办法是iptables规则问题
nfs ~]# iptables -F
docker02 ~]# showmount -e 192.168.1.13
Export list for 192.168.1.13:
/daxiuge888 *

问题二:docker02 ~]# mount 192.168.1.13:/daxiuge /abc
mount.nfs: access denied by server while mounting 192.168.1.13:/daxiuge
解决:挂载路径写错
docker02 ~]# mount 192.168.1.13:/daxiuge888 /abc
[root@docker02 ~]# df -h
192.168.1.13:/daxiuge888  8.0G  1.1G  7.0G   14% /abc


